---
title: "Motorcycle claims - Wasa insurance 1994 to 1998"
author: "Christian Duffau-Rasmussen"
date: "`r Sys.Date()`"
bibliography: litlist.bib
output: github_document
---

```{r, message=FALSE}
library(ggplot2)
library(insuranceData)
library(ROCR)
library(adabag)
library(rpart.plot)
data(dataOhlsson)
```

## The data set

The data comes from the former Swedish insurance company Wasa, and concerns partial casco insurance, for motorcycles. It contains aggregated data on all insurance policies and claims during 1994-1998.

```{r}
head(dataOhlsson)
```

The variables are:

- `agarald` The owners age, between 0 and 99, a numeric vector
- `kon` The owners age, between 0 and 99, a factor with levels K M
- `zon` Geographic zone numbered from 1 to 7, in a standard classification of all Swedish parishes, a numeric vector
- `mcklass` MC class, a classification by the so called EV ratio, defined as (Engine power in kW x 100) / (Vehicle weight in kg + 75), rounded to the nearest lower integer. The 75 kg represent the average driver weight. The EV ratios are divided into seven classes, a numeric vector
- `fordald` Vehicle age, between 0 and 99, a numeric vector
- `bonuskl` Bonus class, taking values from 1 to 7. A new driver starts with bonus class 1; for each claim-free year the bonus class is increased by 1. After the first claim the bonus is decreased by 2; the driver can not return to class 7 with less than 6 consecutive claim free years, a numeric vector
- `duration` the number of policy years, a numeric vector
- `antskad` the number of claims, a numeric vector
- `skadkost` the claim cost, a numeric vector

## Claim classifier

We would like classify which policies would experience claims and which will not. So first we define the variable "has_claim". The models used are all desribed in detail in Elements of Statistical Learning [@elements_of_statistical_learning].

```{r}
dataOhlsson[,"has_claim"] <- factor(dataOhlsson$antskad>0)
```

```{r}
proportion <- table(dataOhlsson$has_claim)/nrow(dataOhlsson)
proportion
```
We see that claims are very rare and only `r sprintf("%.2f%%", proportion[2]*100)` of all policies have claims.

```{r}
set.seed(123)
test.pct <- 0.5
mask <- runif(nrow(dataOhlsson))>test.pct
train.data <- dataOhlsson[mask, ]
test.data <- dataOhlsson[!mask, ]
```
We reserve approximately `r sprintf("%.0f%%", test.pct*100)` of the data set for testing, so the training set conatians `r nrow(train.data)` observations and the remaining `r nrow(test.data)` observations are used for testing.


## Logistic regression

Losgistic regression is a classic technique for classification. For details see [@elements_of_statistical_learning, Section 4.4, pp. 119].

```{r}
model <- glm(has_claim ~ agarald + kon + factor(zon) + factor(mcklass) + fordald + factor(bonuskl) + duration, family=binomial(link='logit'), data=train.data)
summary(model)
```
```{r}
anova(model, test='Chisq')
```

```{r}
p <- predict(model, newdata=test.data, type="response")
pr <- prediction(p, test.data$has_claim)
performance.logistic_reg <- performance(pr, measure = "tpr", x.measure = "fpr")
auc.logistic_reg <- performance(pr, measure = "auc")@y.values[[1]]
```

## CART Decision Tree

See details in [@elements_of_statistical_learning, Section 9.2, pp. 305].

```{r}
model <- rpart(has_claim ~ agarald + kon + factor(zon) + factor(mcklass) + fordald + factor(bonuskl) + duration, data=train.data, method='class',control=rpart.control(minsplit=20, cp=0.0001))
rpart.plot::prp(model,main="Classification Tree for Claims")
```

```{r}
p <- predict(model, newdata=test.data, type="prob")[,2]
pr <- prediction(p, test.data$has_claim)
performance.cart_tree <- performance(pr, measure = "tpr", x.measure = "fpr")
auc.cart_tree <- performance(pr, measure = "auc")@y.values[[1]]
```

## AdaBoost.M1
The AdaBoost.M1 is a learning algorithm which trains a sequence of _M_ descision trees, where the datapoints for training tree number _m_ are weighted according to the predicion accuracy of the previously trained tree (_m-1_). The observations which are correctly classified by the prvious tree get a low weight in the next tree, and predictions which where wrongly classified get a high weight. This way each tree learns different aspects of the feature space, and perform better in practice. The algorithm was first formulated in [@freund1997decision] and is described in [@elements_of_statistical_learning, Section 10.1, pp. 339].

```{r}
model <- adabag::boosting(has_claim ~ agarald + kon + factor(zon) + factor(mcklass) + fordald + factor(bonuskl) + duration, data=train.data, mfinal=100, boos=F, coeflearn='Freund')
par(mar = c(2, 7, 2, 1) + 0.2)
barplot(sort(model$importance), horiz=T, las=2, main="Variable importance")
```

```{r}
p <- adabag::predict.boosting(model, newdata=test.data)$prob[,2]
pr <- prediction(p, test.data$has_claim)
performance.adaboost <- performance(pr, measure = "tpr", x.measure = "fpr")
auc.adaboost <- performance(pr, measure = "auc")@y.values[[1]]
```

## Model performances

```{r}
plot(performance.logistic_reg)
lines(performance.cart_tree@x.values[[1]], performance.cart_tree@y.values[[1]], col=2, new=T)
lines(performance.adaboost@x.values[[1]], performance.adaboost@y.values[[1]], col=3, new=T)
legend(1,0, c('logistic regression', 'CART decision tree', 'Adaboost.M1'), lwd=1, col=1:3, xjust=1, yjust=0)
```

```{r}
data.frame(auc.logistic_reg, auc.adaboost)
```
## References